The idea behind web syndication feeds is not new. In the beginning, there was SGML and Tim Berners-Lee saw that it was good. Written in the mid 1980s with the intention of "enabling the sharing of machine-readable large project documents in government, law and industry" (wiki), the advent of HTML around 1992 and XML in 1996 solidified its place in history.

Although it eventually came to be formally specified, the vast majority of HTML was and is written not against the published standard but tailored to match a few extremely liberal yet subtly incompatible parsers that quickly evolved throughout the 1990s. The explosive growth of the World Wide Web's popularity, and the amount of content stored therein, eventually led to a series of attempts at normalizing its distribution. Early attempts like Apple's Meta Content Framework (around 1995), Microsoft's Channel Definition Format and the W3C's Resource Description Framework (1997) eventually culminated in the Really Simple Syndication format in 1999. RSS quickly became a defacto standard and by 2002 major organizations like the New York Times were starting to roll out their own implementations. 

As programmers we are trained to reduce the complex reality surrounding us into manageable atomic bits and pieces we are capable of reasoning about and so it is easy to forget the fact that any protocol or interface by definition consists of a series of compromises between its designers. The RSS format is no different, and the abbreviated summary above doesn't do justice to the amount of work between working groups and standardizing bodies that culminated in its creation. The format was immediately marred by competing interests, loose authorship authority stemming from Netscape's sale to AOL in 1999 and a wide range of arguments on what exactly constitutes a 'Well Formed Log Entry'(http://www.intertwingly.net/blog/1472.html).

By early 2004 there were seven incompatible versions of RSS(http://diveintomark.org/archives/2004/02/04/incompatible-rss) which now also competed against a similar, yet more flexible, format called Atom. Atom was promptly adopted by Google and pushed across all of their properties, immediately assuring it a significant market share. The exact differences between the formats are, for our purposes, largely irrelevant and interesting to no one except XML aficionados and the authors of feed parsing libraries (blessed be their souls). Much like with HTML, very few people have bothered reading any of the specs and instead target their preferred parser implementations. 

What is significant is that it slowly became necessary for web authors to offer some kind of syndication mechanism for their readers. The omnipresence of a syndicated, machine readable feed enabled a variety of applications to take root. The most significant of these is probably podcasting, which embeds the url of an audio file and allows client applications to consume them on a regularly ocurring schedule. Invented in the early 2000s, the medium rose to prominence around 2004 and has since become a staple of online audio distribution. Today, it is almost unthinkable for any website with regularly updating content to not provide a syndication feed. Indeed, in a pointlessly confusing move, many choose to offer their users the choice of two or three different formats. 

For the purposes of this paper, 'RSS feeds' shall refer to all commonly used machine readable syndication formats.

The Problem

Despite their ubiquity, almost no one uses RSS feeds. All rigorous interest in its adoption rate seems to have waned by 2005, but by 2008 it was estimated that at most 11% of internet users made some use of it (http://profy.com/2008/10/20/low-rss-adoption-is-it-about-tools-or-needs/). As a result, all serious use of feed readers seems to be limited to the technically savvy.

This is unfortunate, as feed aggregators can be immensely useful tools. They concentrate content into easily digestible chunks, significantly lowering the opportunity cost associated with consuming content on the internet. Freed from having to manually organize and mantain lists of bookmarks, users who before might follow a handful of sites at most became free to consolidate their reading habits and indulge in dozens or even hundreds of feeds.

Most feed readers out on the market today are fairly straightforward and present the user to what amounts to a queue, roughly modeled on the inbox metaphor email clients use. All content items are treated the same and are usually sorted by date and some user-input categorization (usually, by grouping different sources together). This works just fine if we are dealing with a limited number of subscriptions, but past a certain threshold (say, 200 regularly updating blogs), this quickly turns out to be unmanageable and before long it begins to impose a large cognitive strain on its users.

At this stage, a curious psychological event occurs. Although the burden imposed by an excessive quantity of feed subscriptions grows roughly linearly with the quantity of subscriptions, there seems to exist an abrupt cut off point where the cost associated with keeping up to date exceeds the perceived benefits of subscription. Faced with an overwhelming quantity of information to process on a daily basis, it seems that most users prefer to simply give up entirely on the process instead of investing even more time reducing their commitment(http://www.readwriteweb.com/archives/rss_reader_market_in_disarray.php http://www.techcrunchit.com/2009/05/05/rest-in-peace-rss/ http://al3x.net/2009/07/18/fever-and-the-future-of-feed-readers.html). What began as a panacea slowly grows into an affliction.

However, people have a nearly insatiable desire for new information. So, having freed up several hours what have these power users turned to instead? The answer lies in the filtering capacity of social networks. Twitter and Facebook, to name just the most significant ones, are in effect used as news distribution networks, with the caveat that information selected by one's peers is significantly more likely to be of high value. Yet even these mediums suffer from the same predicament: the total amount of subscriptions are doomed to grow until they too become unmanageable. In the end, they act just like any other news aggregator, and are bound to suffer from the same limitations.


The problem thus can be roughly defined as that of an excess of volume. Not all information is created equally. Not everything out there will prove relevant or interesting to any individual person; furthermore, as the total amount of content grows, the amount of undesirable content is likely to grow much faster than the total quantity of relevant information. Seeing as we are unable to distinguish between more and less valuable information before we consume it, uninteresting content quickly becomes a significant time sink.


Here I seek to argue that one of the key issues underlying this problem is that there is a gap between the ability of our tools to work with the way we actually read content. The inbox metaphor, applied to feed aggregators, does not scale and past a certain threshold is fundamentally broken. The experience of reading as entertainment turns out to be substantially different from the experience of reading email. Email commands all of our attention; even if we choose to ignore most of the messages we receive, it's still fundamentally important that we receive them in the first place. 

Reading, as entertainment, does not usually suffer from this problem. Take a newspaper, for example. Few people have the time or the interest to read one from cover to cover. Editors select the most important stories and concentrate them in the very first or very last few pages. While we may carefully read the front page and the editorial section, we're very likely to only skim the content in between. Unfortunately for us, most feed readers operate on the assumption that all content is equally worthy of our attention. Instead of being presented with the most relevant posts of the day, items are queued up and stored indefinitely until we get around to dismissing them from the system. 

So, how can we design a better system? 
- unobtrusive UI
 minimize friction, improve readability
- intelligent filtering
 reduce overall volume
 how can we achieve this?
  fairly simple text classification techniques, like bayes' and a few other heuristics allow us to make mostly correct inferences about what is and what is not important
  underlying assumption: most everything is noise, or not dramatically important - false positives AND false negatives have minimal impact. Ideally minimize false positives, of course. 

FEATURE SELECTION
- Used Paul Graham's spam essays as a basis from which to improve upon
  meaningful ways in which feeds differ from spam:
  feeds are effectively white lists, so more concerned with identifying meaningful topics - variation between noise and signal isn't as pronounced
  
  unlike email, there isn't any highly-significant metadata like email headers, or the presence of images, or complicated markup or so on.

- the meaning of markup
  decision to weigh titles and links and urls more heavily. this is achieved by parsing the text and doubling or tripling the occurance in the corpus of these significant tokens.

  link text, link urls, img alt text, image urls, titles and authors

  experimentation with bigrams and trigrams for various features and the entire corpus

  {{{ great graph opportunity here }}}


  

{{{content quality largely self selecting

- mention how it becomes overwhelming
- mention how people in turn are dropping it out
- software wastes too much of our time; the amount of content out on the internet greatly outpaces our capacity to consume it
}}}

initially, the poor performance of the classifiers is disheartening, but in regards to our original problem (i.e. reducing the total VOLUME of incoming entries) it is far from useless. Different classifiers with different performance characteristics can be introduced depending on the user's preferences. 

problem is an analog of on-line supervised spam filtering (cormack lynam)

implementation
  Built entirely upon the Ruby on Rails framework backed by a MySQL database and supplied with data through two cron-scheduled daemons. The presentation layer can be easily modified to accomodate the current variety of display devices now commonly available (mobile devices, tablet devices, personal computers).

  A few design goals underlied the overal interface: minimizing friction and optimizing ease of consumption. A common complaint amongst rss feed reader users is the feeling of burden associated with the inbox model most feed readers present - frustration mounts when one is faced with an overwhelming quantity of items to consume. Therefore, it was a conscious design goal to hide the overall quantity of items left in the queue.

  Yet, experimentation with a limited set of users suggests that some sense of continuity, or the feeling of accomplishment that comes with finishing a stream of content is important. If an implied goal of the tool is to *reduce* overall levels of procrastination, it does us no good to confront users with a seemingly infinite stream of content. 
  
  This opens the door to a variety of interesting experiments and increasing customizations. 

limitations / further research
  - twitter
  twitter is a ripe arena to apply some of these concepts. Research involving SMS spam (a practically inexistent problem in the West but apparently rife throughout Asia) suggests that even equally simple text classification algorithms can reap significant dividends.

  At the time of writing, the only client performing any kind of filtering seems limited to eliminating specific keywords, and the limited feature pool probably suffers from the same limitations described with rss feeds at large described in this paper.

  - including social graph to perform further optimizations
  


technical issues
  - the problem with rss
  as with any protocol, there is a wide divergence between the technical specification and the myriad implementations, hacked together solutions and use cases available in the wild. 

  Issues surrounding malformed rss feeds are completely invisible to the end user
  
  As a result any useful parser must take Postel's Law to heart and be particularly forgiving. Fortunately, most of the heavy lifting can be accomodated by a wide range of available libraries. A non trivial quantity of work goes into guessing the intent of the feed author; common issues include the lack of url attributes, missing title attributes and the use of relative URIs inside resource urls. This project utilized so and so's Feedzirra, though the state of the art is undeniably Mark Pilgrim's feed_parser (which unfortunately, is written in python and thus not suitable for this project).

  - the problem with markup

  A lot of effort was then spent on massaging the input text and appropriately stripping out punctuation. 

  - platform woes
  Ruby on Rails is an excellent prototyping environment that, for the experienced web developer, abstracts away many trivial concerns and greatly speeds up development. Unfortunately, MRI, the canonical Ruby interpreter, is still something of an immature platform and consequently a non trivial quantity of time was spent chasing nigh-invisible memory leaks and namespace collisions.

  - the problem with classifiers

  performance varied wildly with algorithms and individual libraries. at the time of writing the exact cause is unknown but in theory our high tolerance of false positives and false negatives allows us to experiment with different rates of filtering - different text classifiers with different precision metrics can be used to filter more or less aggressively, i.e. crm114 flags about ~20% of all entries as noise, but our custom naive-bayes flags ~80% as noise. Using {{insert formula}} to use as a confidence ratio, we can adjust the rate of flow of entries into the stream.


  
goals of project
explore the use of spam filtering techniques on rss feeds
  custom reader UI is an increasingly popular trend, i.e. readability and instapaper
  web is a grand repository of information, but is poorly suited for skimming content - not a problem back when only a few sites might command our attention, but overwhelming today

  from a user perspective, similar problem: overwhelmed with low ROI (of attention) content
  increasing trend amongst the tech literati to abandon the format due to this problem

proposed solutions: unobtrusive UI coupled with machine learning of one form or another
  differences in problems: blog content is almost by definition ephemeral; unlike email it is not essential that it all be consumed, i.e. extremely high tolerance to 'missing out'.
  allows us to approach the problem from the perspective of user-life-management - instead of an inbox approach, with defined remaining capacities, better to manage people's reading for them; increase filtering according to available time allotted

  differences in approach
  different classifiers yield different precision metrics which can be utilized for slightly different types of filtering
