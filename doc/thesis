The idea behind web syndication feeds is not new. In the beginning, there was SGML and Tim Berners-Lee saw that it was good. Written in the mid 1980s with the intention of "enabling the sharing of machine-readable large project documents in government, law and industry" (wiki), the advent of HTML around 1992 and XML in 1996 solidified its place in history.

Although it eventually came to be formally specified, the vast majority of HTML was and is written not against the published standard but tailored to match a few extremely liberal yet subtly incompatible parsers that quickly evolved throughout the 1990s. The explosive growth of the World Wide Web's popularity, and the amount of content stored therein, eventually led to a series of attempts at normalizing its distribution. Early attempts like Apple's Meta Content Framework (around 1995), Microsoft's Channel Definition Format and the W3C's Resource Description Framework (1997) eventually culminated in the Really Simple Syndication format in 1999. RSS quickly became a defacto standard and by 2002 major organizations like the New York Times were starting to roll out their own implementations. 

As programmers we are trained to reduce the complex reality surrounding us into manageable atomic bits and pieces we are capable of reasoning about and so it is easy to forget the fact that any protocol or interface by definition consists of a series of compromises between its designers. The RSS format is no different, and the abbreviated summary above doesn't do justice to the amount of work between working groups and standardizing bodies that culminated in its creation. The format was immediately marred by competing interests, loose authorship authority stemming from Netscape's sale to AOL in 1999 and a wide range of arguments on what exactly constitutes a 'Well Formed Log Entry'(http://www.intertwingly.net/blog/1472.html).

By early 2004 there were seven incompatible versions of RSS(http://diveintomark.org/archives/2004/02/04/incompatible-rss) which now also competed against a similar, yet more flexible, format called Atom. Atom was promptly adopted by Google and pushed across all of their properties, immediately assuring it a significant market share. The exact differences between the formats are, for our purposes, largely irrelevant and interesting to no one except XML aficionados and the authors of feed parsing libraries (blessed be their souls). Much like with HTML, very few people have bothered reading any of the specs and instead target their preferred parser implementations. 

What is significant is that it slowly became necessary for web authors to offer some kind of syndication mechanism for their readers. The omnipresence of a syndicated, machine readable feed enabled a variety of applications to take root. The most significant of these is probably podcasting, which embeds the url of an audio file and allows client applications to consume them on a regularly ocurring schedule. Invented in the early 2000s, the medium rose to prominence around 2004 and has since become a staple of online audio distribution. Today, it is almost unthinkable for any website with regularly updating content to not provide a syndication feed. Indeed, in a pointlessly confusing move, many choose to offer their users the choice of two or three different formats. 

For the purposes of this paper, 'RSS feeds' shall refer to all commonly used machine readable syndication formats.

The Problem

Despite their ubiquity, almost no one uses RSS feeds. All rigorous interest in its adoption rate seems to have waned by 2005, but by 2008 it was estimated that at most 11% of internet users made some use of it (http://profy.com/2008/10/20/low-rss-adoption-is-it-about-tools-or-needs/). As a result, all serious use of feed readers seems to be limited to the technically savvy.

This is unfortunate, as feed aggregators can be immensely useful tools. They concentrate content into easily digestible chunks, significantly lowering the opportunity cost associated with consuming content on the internet. Freed from having to manually organize and mantain lists of bookmarks, users who before might follow a handful of sites at most became free to consolidate their reading habits and indulge in dozens or even hundreds of feeds.

Most feed readers out on the market today are fairly straightforward and present the user to what amounts to a queue, roughly modeled on the inbox metaphor email clients use. All content items are treated the same and are usually sorted by date and some user-input categorization (usually, by grouping different sources together). This works just fine if we are dealing with a limited number of subscriptions, but past a certain threshold (say, 200 regularly updating blogs), this quickly turns out to be unmanageable and before long it begins to impose a large cognitive strain on its users.

At this stage, a curious psychological event occurs. Although the burden imposed by an excessive quantity of feed subscriptions grows roughly linearly with the amount of feeds subscribed to, there seems to exist an abrupt cut off point where the cost associated with keeping up to date exceeds the perceived benefits of subscription. Faced with an overwhelming quantity of information to process on a daily basis, it seems that many if not most users simply give up entirely on the process(http://www.readwriteweb.com/archives/rss_reader_market_in_disarray.php http://www.techcrunchit.com/2009/05/05/rest-in-peace-rss/ http://al3x.net/2009/07/18/fever-and-the-future-of-feed-readers.html). What began as a panacea slowly grows into an affliction.


The problem thus can be roughly defined as that of an excess of volume. Not all information is created equally. Not everything out there will prove relevant or interesting to any individual person; furthermore, as the total amount of content grows, the amount of undesirable content is likely to grow much faster than the total quantity of relevant information. Seeing as we are unable to distinguish between more and less valuable information before we consume it, uninteresting content quickly becomes a significant time sink.

In the end, people have a nearly insatiable desire for new information. So, having freed up several hours what have these power users turned to instead? The answer lies in the filtering capacity of social networks. Twitter and Facebook, to name just the most significant ones, are in effect used as news distribution networks, with the caveat that information selected by one's peers is vastly more likely to be of high value. 


The trend to move  twitter or other social networks as a content aggregator 


inbox metaphor broken - what is needed is a stream, not a queue
paradoxical problem with overabundance of choice

feature selection

content quality largely self selecting

- mention how it becomes overwhelming
- mention how people in turn are dropping it out
- software wastes too much of our time; the amount of content out on the internet greatly outpaces our capacity to consume it
- 

initially, the poor performance of the classifiers is disheartening, but in regards to our original problem (i.e. reducing the total VOLUME of incoming entries) it is far from useless. Different classifiers with different performance characteristics can be introduced depending on the user's preferences. 

problem is an analog of on-line supervised spam filtering (cormack lynam)

implementation
  Built entirely upon the Ruby on Rails framework backed by a MySQL database and supplied with data through two cron-scheduled daemons. The presentation layer can be easily modified to accomodate the current variety of display devices now commonly available (mobile devices, tablet devices, personal computers).

  A few design goals underlied the overal interface: minimizing friction and optimizing ease of consumption. A common complaint amongst rss feed reader users is the feeling of burden associated with the inbox model most feed readers present - frustration mounts when one is faced with an overwhelming quantity of items to consume. Therefore, it was a conscious design goal to hide the overall quantity of items left in the queue.

  Yet, experimentation with a limited set of users suggests that some sense of continuity, or the feeling of accomplishment that comes with finishing a stream of content is important. If an implied goal of the tool is to *reduce* overall levels of procrastination, it does us no good to confront users with a seemingly infinite stream of content. 
  
  This opens the door to a variety of interesting experiments and increasing customizations. 

limitations / further research
  - twitter
  twitter is a ripe arena to apply some of these concepts. Research involving SMS spam (a practically inexistent problem in the West but apparently rife throughout Asia) suggests that even equally simple text classification algorithms can reap significant dividends.

  At the time of writing, the only client performing any kind of filtering seems limited to eliminating specific keywords, and the limited feature pool probably suffers from the same limitations described with rss feeds at large described in this paper.


technical issues
  - the problem with rss
  as with any protocol, there is a wide divergence between the technical specification and the myriad implementations, hacked together solutions and use cases available in the wild. 

  Issues surrounding malformed rss feeds are completely invisible to the end user
  
  As a result any useful parser must take Postel's Law to heart and be particularly forgiving. Fortunately, most of the heavy lifting can be accomodated by a wide range of available libraries. A non trivial quantity of work goes into guessing the intent of the feed author; common issues include the lack of url attributes, missing title attributes and the use of relative URIs inside resource urls. This project utilized so and so's Feedzirra, though the state of the art is undeniably Mark Pilgrim's feed_parser (which unfortunately, is written in python and thus not suitable for this project).

  - platform woes
  Ruby on Rails is an excellent prototyping environment that, for the experienced web developer, abstracts away many trivial concerns and greatly speeds up development. Unfortunately, MRI, the canonical Ruby interpreter, is still something of an immature platform and consequently a non trivial quantity of time was spent chasing nigh-invisible memory leaks and namespace collisions.

  - the problem with classifiers


  
goals of project
explore the use of spam filtering techniques on rss feeds
  custom reader UI is an increasingly popular trend, i.e. readability and instapaper
  web is a grand repository of information, but is poorly suited for skimming content - not a problem back when only a few sites might command our attention, but overwhelming today

  from a user perspective, similar problem: overwhelmed with low ROI (of attention) content
  increasing trend amongst the tech literati to abandon the format due to this problem

proposed solutions: unobtrusive UI coupled with machine learning of one form or another
  differences in problems: blog content is almost by definition ephemeral; unlike email it is not essential that it all be consumed, i.e. extremely high tolerance to 'missing out'.
  allows us to approach the problem from the perspective of user-life-management - instead of an inbox approach, with defined remaining capacities, better to manage people's reading for them; increase filtering according to available time allotted

  differences in approach
  different classifiers yield different precision metrics which can be utilized for slightly different types of filtering
